{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import pyBigWig\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import time \n",
    "import numpy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from logging_myin25 import Logger\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpnet.py\n",
    "# Author: Jacob Schreiber <jmschreiber91@gmail.com>\n",
    "\n",
    "\"\"\"\n",
    "This module contains a reference implementation of BPNet that can be used\n",
    "or adapted for your own circumstances. The implementation takes in a\n",
    "stranded control track and makes predictions for stranded outputs.\n",
    "\"\"\"\n",
    "\n",
    "import time \n",
    "import numpy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from logging_myin25 import Logger\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "class DefinitelyNotBPNet(torch.nn.Module):\n",
    "    \"\"\"A basic model model with count prediction.\n",
    "\n",
    "    This model consists of a single Dense layer and an output layer!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_filters: int, optional\n",
    "        The number of filters to use per convolution. Default is 64.\n",
    "\n",
    "    name: str or None, optional\n",
    "        The name to save the model to during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_marks, name=None, alpha=1):\n",
    "        super(DefinitelyNotBPNet, self).__init__()\n",
    "\n",
    "        self.name = name or \"/users/myin25/projects/celltype_specificity/models/definitelynotbpnet_{}\".format(datetime.now())\n",
    "        self.alpha = alpha\n",
    "        '''\n",
    "        self.fc1 = torch.nn.Linear(8, 32)\n",
    "        self.relu1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(32, 64)\n",
    "        self.relu2 = torch.nn.Sigmoid()\n",
    "        self.fc3 = torch.nn.Linear(64, 16)\n",
    "        self.relu3 = torch.nn.Sigmoid()\n",
    "        self.fc4 = torch.nn.Linear(16, 1)'''\n",
    "\n",
    "        # self.convi = torch.nn.Linear(n_marks, 1)\n",
    "        self.convi = torch.nn.Conv1d(n_marks, 64, kernel_size=21)\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "\n",
    "        self.logger = Logger([\"Epoch\", \"Iteration\", \"Training Time\",\n",
    "            \"Training MNLL Loss\", \"Training Count MSE\", \"Val MNLL Loss\",\"Validation Count Pearson\", \n",
    "            \"Validation Count MSE\", \"Saved?\"], verbose=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, X_ctl=None):\n",
    "        # counts prediction\n",
    "        '''x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)'''\n",
    "        \n",
    "        #print('shape before prediction', x.shape)\n",
    "        x = self.convi(x)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        x = self.linear(x)\n",
    "        #print('prediction shape', x.shape)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, X, X_ctl=None, batch_size=64, verbose=False):\n",
    "        \"\"\"Make predictions for a large number of examples.\n",
    "\n",
    "        This method will make predictions for a number of examples that exceed\n",
    "        the batch size. It is similar to the forward method in terms of inputs \n",
    "        and outputs, but will run wrapped with `torch.no_grad()` to speed up\n",
    "        computation and prevent information leakage into the model.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.tensor, shape=(-1, 4, length)\n",
    "            The one-hot encoded batch of sequences.\n",
    "\n",
    "        X_ctl: torch.tensor or None, shape=(-1, n_strands, length)\n",
    "            A value representing the signal of the control at each position in \n",
    "            the sequence. If no controls, pass in None. Default is None.\n",
    "\n",
    "        batch_size: int, optional\n",
    "            The number of examples to run at a time. Default is 64.\n",
    "\n",
    "        verbose: bool\n",
    "            Whether to print a progress bar during predictions.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_profile: torch.tensor, shape=(-1, n_strands, out_length)\n",
    "            The output predictions for each strand trimmed to the output\n",
    "            length.\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"X shape\", X.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            starts = numpy.arange(0, X.shape[0], batch_size)\n",
    "            ends = starts + batch_size\n",
    "\n",
    "            y_profiles, y_counts = [], []\n",
    "            for start, end in tqdm(zip(starts, ends), disable=not verbose):\n",
    "                X_batch = X[start:end].cuda()\n",
    "                #print(\"X_batch shape\", X_batch.shape)\n",
    "                y_prof_ = self(X_batch)\n",
    "                y_prof_ = y_prof_.cpu()\n",
    "\n",
    "                y_profiles.append(y_prof_)\n",
    "\n",
    "            y_profiles = torch.cat(y_profiles)\n",
    "            return y_profiles\n",
    "\n",
    "    def fit(self, training_data, optimizer, X_valid=None, X_ctl_valid=None, \n",
    "        y_valid=None, max_epochs=100, batch_size=64, validation_iter=100, \n",
    "        early_stopping=None, verbose=True):\n",
    "        \"\"\"Fit the model to data and validate it periodically.\n",
    "\n",
    "        This method controls the training of a BPNet model. It will fit the\n",
    "        model to examples generated by the `training_data` DataLoader object\n",
    "        and, if validation data is provided, will periodically validate the\n",
    "        model against it and return those values. The periodicity can be\n",
    "        controlled using the `validation_iter` parameter.\n",
    "\n",
    "        Two versions of the model will be saved: the best model found during\n",
    "        training according to the validation measures, and the final model\n",
    "        at the end of training. Additionally, a log will be saved of the\n",
    "        training and validation statistics, e.g. time and performance.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: torch.utils.data.DataLoader\n",
    "            A generator that produces examples to train on. If n_control_tracks\n",
    "            is greater than 0, must product two inputs, otherwise must produce\n",
    "            only one input.\n",
    "\n",
    "        optimizer: torch.optim.Optimizer\n",
    "            An optimizer to control the training of the model.\n",
    "\n",
    "        X_valid: torch.tensor or None, shape=(n, 4, 2114)\n",
    "            A block of sequences to validate on periodically. If None, do not\n",
    "            perform validation. Default is None.\n",
    "\n",
    "        X_ctl_valid: torch.tensor or None, shape=(n, n_control_tracks, 2114)\n",
    "            A block of control sequences to validate on periodically. If\n",
    "            n_control_tracks is None, pass in None. Default is None.\n",
    "\n",
    "        y_valid: torch.tensor or None, shape=(n, n_outputs, 1000)\n",
    "            A block of signals to validate against. Must be provided if\n",
    "            X_valid is also provided. Default is None.\n",
    "\n",
    "        max_epochs: int\n",
    "            The maximum number of epochs to train for, as measured by the\n",
    "            number of times that `training_data` is exhausted. Default is 100.\n",
    "\n",
    "        batch_size: int\n",
    "            The number of examples to include in each batch. Default is 64.\n",
    "\n",
    "        validation_iter: int\n",
    "            The number of batches to train on before validating against the\n",
    "            entire validation set. When the validation set is large, this\n",
    "            enables the total validating time to be small compared to the\n",
    "            training time by only validating periodically. Default is 100.\n",
    "\n",
    "        early_stopping: int or None\n",
    "            Whether to stop training early. If None, continue training until\n",
    "            max_epochs is reached. If an integer, continue training until that\n",
    "            number of `validation_iter` ticks has been hit without improvement\n",
    "            in performance. Default is None.\n",
    "\n",
    "        verbose: bool\n",
    "            Whether to print out the training and evaluation statistics during\n",
    "            training. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"starting fitting\")\n",
    "        \n",
    "        if X_valid is not None:\n",
    "            X_valid = X_valid.cuda()\n",
    "            y_valid = y_valid.sum(dim=(-1, -2)).cuda()\n",
    "            \n",
    "            #X_valid = X_valid.sum(dim=-1).cuda()\n",
    "            #print(\"y_valid.shape\", y_valid.shape)\n",
    "            #print(\"y_valid.shape after\", y_valid.shape)\n",
    "            #print(\"X_valid.shape\", X_valid.shape)\n",
    "            #print(\"y_valid.shape\", y_valid.shape)\n",
    "\n",
    "        iteration = 0\n",
    "        early_stop_count = 0\n",
    "        best_loss = float(\"inf\")\n",
    "        self.logger.start()\n",
    "\n",
    "        #print(\"starting to train\")\n",
    "        for epoch in range(max_epochs):\n",
    "            tic = time.time()\n",
    "\n",
    "            for seqs, X, y in training_data:\n",
    "                X, y = X.cuda(), y.sum(dim=(-1, -2)).cuda()\n",
    "                #X, y = X.sum(dim=-1).cuda(), y.sum(dim=(-1, -2)).cuda()\n",
    "                #print(\"X numpy size\", X.numpy().nbytes)\n",
    "                #print(\"y numpy size\", y.numpy().nbytes)\n",
    "                #print(\"X.shape\", X.shape)\n",
    "                #print(\"y.shape\", y.shape)\n",
    "\n",
    "                # Clear the optimizer and set the model to training mode\n",
    "                optimizer.zero_grad()\n",
    "                self.train()\n",
    "\n",
    "                # Run forward pass\n",
    "                y_prof = self(X).squeeze()\n",
    "                #print(\"y_prof\", y_prof.shape)\n",
    "                #print(\"y prediction numpy size\", y_prof.numpy().nbytes)\n",
    "\n",
    "                #print(\"predicted, going to evaluate losses.\")\n",
    "                # Calculate the profile and count losses\n",
    "                #print('shape comparison')\n",
    "                #print(y_prof.shape, y.shape)\n",
    "                t_prof_loss = MNLLLoss(y_prof, y).mean()\n",
    "                t_count_loss = log1pMSELoss(torch.sum(y_prof, dim=-1), torch.sum(y, dim=-1)).mean()\n",
    "                \n",
    "                loss = t_prof_loss + self.alpha * t_count_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if verbose and iteration % validation_iter == 0:\n",
    "                    train_time = time.time() - tic\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        self.eval()\n",
    "                        \n",
    "                        tic = time.time()\n",
    "                        y_profs = self.predict(X_valid, X_ctl_valid)\n",
    "                        # print(\"y_profs.shape\", y_profs.shape)\n",
    "                        y_counts = torch.sum(y_profs, dim=-1)\n",
    "                        \n",
    "                        #print(\"y val actual profs numpy size\", y_valid.numpy().nbytes)\n",
    "                        #print(\"y val predicted profs numpy size\", y_profs.numpy().nbytes)\n",
    "                        \n",
    "                        '''print(\"validation predicted profs\", y_valid)\n",
    "                        print(\"validation predicted profs\", y_profs)'''\n",
    "                        \n",
    "                        # MSE\n",
    "                        '''print('y_valid', y_valid)\n",
    "                        print('y_valid min', torch.min(y_valid))\n",
    "                        print('y_valid max', torch.max(y_valid))\n",
    "                        print('y_counts', y_counts)\n",
    "                        print('y_counts min', torch.min(y_counts))\n",
    "                        print('y_counts max', torch.max(y_counts))'''\n",
    "                        \n",
    "                        log_true = torch.log1p(y_valid)\n",
    "                        \n",
    "                        #print(\"log_true before\", y_valid.shape)\n",
    "                        #log_true = torch.sum(torch.log(y_valid+1), dim=-1)\n",
    "                        log_true = log_true.cpu()\n",
    "                        #print(\"log_true after\", log_true.shape)\n",
    "                        \n",
    "                        '''print('log_true', log_true)'''\n",
    "                        '''plt.scatter(log_true,y_counts.squeeze())\n",
    "                        plt.show()'''\n",
    "                        \n",
    "                        count_mse = torch.square(log_true[..., None] - y_counts)\n",
    "                        count_mse = torch.mean(count_mse.squeeze(), dim=-1)\n",
    "                        \n",
    "                        '''print(count_mse)\n",
    "                        print(any(torch.isnan(count_mse)))'''\n",
    "                        '''plt.hist(count_mse)\n",
    "                        plt.show()'''\n",
    "                        \n",
    "                        # count pearson\n",
    "                        y_counts = y_counts.squeeze()\n",
    "                        '''plt.scatter(log_true, y_counts)\n",
    "                        plt.show()\n",
    "                        print('comparison of shapes')\n",
    "                        print(log_true.shape, y_counts.shape)\n",
    "                        print(torch.min(log_true))\n",
    "                        print(torch.max(log_true))\n",
    "                        print(torch.mean(log_true))\n",
    "                        plt.hist(log_true)\n",
    "                        plt.show()\n",
    "                        print(any(torch.isnan(y_counts)))\n",
    "                        print(torch.min(y_counts))\n",
    "                        print(torch.max(y_counts))\n",
    "                        print(torch.mean(y_counts))\n",
    "                        plt.hist(y_counts)\n",
    "                        plt.show()'''\n",
    "                        \n",
    "                        count_corr = pearson_corr(log_true, y_counts)\n",
    "                        \n",
    "                        y_valid = y_valid.cpu()\n",
    "                        #X_valid = X_valid.cpu()\n",
    "                        #print('comparison of shapes 2')\n",
    "                        #print(y_valid.shape, y_counts.shape)\n",
    "                        #prof_loss = MNLLLoss(y_profs, y_valid).mean().item()\n",
    "                        count_loss = log1pMSELoss(y_counts, y_valid.sum(dim=-1).mean())\n",
    "                        prof_loss = count_loss\n",
    "                        \n",
    "                        valid_time = time.time() - tic\n",
    "                        #valid_loss = count_mse.mean()\n",
    "                        valid_loss = count_loss\n",
    "                        count_loss = count_loss.item()\n",
    "\n",
    "                        '''print('count_loss_')\n",
    "                        print(count_loss)\n",
    "                        print('count_corr')\n",
    "                        print(count_corr)\n",
    "                        print('nan_to_num')\n",
    "                        print(numpy.nan_to_num(count_corr).mean())\n",
    "                        print('count_mse')\n",
    "                        print(count_mse.mean().item())\n",
    "                        print('saved')\n",
    "                        print((valid_loss < best_loss).item())'''\n",
    "                        \n",
    "                        t_count_loss = t_count_loss.item()\n",
    "                        \n",
    "                        t_prof_loss = t_prof_loss.cpu().item()\n",
    "                        '''print(t_prof_loss.device)\n",
    "                        print(prof_loss.device)\n",
    "                        print(count_mse.device)\n",
    "                        print(valid_loss.device)'''\n",
    "                        self.logger.add([epoch, iteration, train_time, \n",
    "                            0, t_count_loss, 0,\n",
    "                            numpy.nan_to_num(count_corr).mean(), \n",
    "                            count_mse.mean().item(),\n",
    "                            (valid_loss < best_loss).item()])\n",
    "                        \n",
    "                        #print(\"name\", type(self.name))\n",
    "                        self.logger.save(\"{}.log\".format(self.name))\n",
    "                        \n",
    "                        if valid_loss < best_loss:\n",
    "                            torch.save(self, \"{}.torch\".format(self.name))\n",
    "                            best_loss = valid_loss\n",
    "                            early_stop_count = 0\n",
    "                        else:\n",
    "                            early_stop_count += 1\n",
    "\n",
    "                # return\n",
    "                \n",
    "                # Extract the profile loss for logging\n",
    "                # loss = count_loss.item()\n",
    "\n",
    "                if early_stopping is not None and early_stop_count >= early_stopping:\n",
    "                    break\n",
    "\n",
    "                iteration += 1\n",
    "                \n",
    "                '''print()\n",
    "                print()'''\n",
    "\n",
    "            if early_stopping is not None and early_stop_count >= early_stopping:\n",
    "                break\n",
    "\n",
    "        torch.save(self, \"{}.final.torch\".format(self.name))\n",
    "\n",
    "def pearson_corr(arr1, arr2):\n",
    "    \"\"\"The Pearson correlation between two tensors across the last axis.\n",
    "\n",
    "    Computes the Pearson correlation in the last dimension of `arr1` and `arr2`.\n",
    "    `arr1` and `arr2` must be the same shape. For example, if they are both\n",
    "    A x B x L arrays, then the correlation of corresponding L-arrays will be\n",
    "    computed and returned in an A x B array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr1: torch.tensor\n",
    "        One of the tensor to correlate.\n",
    "\n",
    "    arr2: torch.tensor\n",
    "        The other tensor to correlation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    correlation: torch.tensor\n",
    "        The correlation for each element, calculated along the last axis.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(arr1.shape)\n",
    "    #print(torch.min(arr1), torch.max(arr1))\n",
    "    #print(arr2.shape)\n",
    "    #print(torch.min(arr2), torch.max(arr2))\n",
    "    \n",
    "    mean1 = torch.mean(arr1, axis=-1).unsqueeze(-1)\n",
    "    mean2 = torch.mean(arr2, axis=-1).unsqueeze(-1)\n",
    "    dev1, dev2 = arr1 - mean1, arr2 - mean2\n",
    "\n",
    "    sqdev1, sqdev2 = torch.square(dev1), torch.square(dev2)\n",
    "    numer = torch.sum(dev1 * dev2, axis=-1)  # Covariance\n",
    "    var1, var2 = torch.sum(sqdev1, axis=-1), torch.sum(sqdev2, axis=-1)  # Variances\n",
    "    denom = torch.sqrt(var1 * var2)\n",
    "   \n",
    "    # Divide numerator by denominator, but use 0 where the denominator is 0\n",
    "    correlation = torch.zeros_like(numer)\n",
    "    correlation[denom != 0] = numer[denom != 0] / denom[denom != 0]\n",
    "    return correlation\n",
    "\n",
    "def log1pMSELoss(log_predicted_counts, true_counts):\n",
    "\t\"\"\"A MSE loss on the log(x+1) of the inputs.\n",
    "\n",
    "\tThis loss will accept tensors of predicted counts and a vector of true\n",
    "\tcounts and return the MSE on the log of the labels. The squared error\n",
    "\tis calculated for each position in the tensor and then averaged, regardless\n",
    "\tof the shape.\n",
    "\n",
    "\tNote: The predicted counts are in log space but the true counts are in the\n",
    "\toriginal count space.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tlog_predicted_counts: torch.tensor, shape=(n, ...)\n",
    "\t\tA tensor of log predicted counts where the first axis is the number of\n",
    "\t\texamples. Important: these values are already in log space.\n",
    "\n",
    "\ttrue_counts: torch.tensor, shape=(n, ...)\n",
    "\t\tA tensor of the true counts where the first axis is the number of\n",
    "\t\texamples.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tloss: torch.tensor, shape=(n, 1)\n",
    "\t\tThe MSE loss on the log of the two inputs, averaged over all examples\n",
    "\t\tand all other dimensions.\n",
    "\t\"\"\"\n",
    "\n",
    "\tlog_true = torch.log(true_counts+1)\n",
    "\treturn torch.mean(torch.square(log_true - log_predicted_counts), dim=-1)\n",
    "\n",
    "def MNLLLoss(logps, true_counts):\n",
    "\t\"\"\"A loss function based on the multinomial negative log-likelihood.\n",
    "\n",
    "\tThis loss function takes in a tensor of normalized log probabilities such\n",
    "\tthat the sum of each row is equal to 1 (e.g. from a log softmax) and\n",
    "\tan equal sized tensor of true counts and returns the probability of\n",
    "\tobserving the true counts given the predicted probabilities under a\n",
    "\tmultinomial distribution. Can accept tensors with 2 or more dimensions\n",
    "\tand averages over all except for the last axis, which is the number\n",
    "\tof categories.\n",
    "\n",
    "\tAdapted from Alex Tseng.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tlogps: torch.tensor, shape=(n, ..., L)\n",
    "\t\tA tensor with `n` examples and `L` possible categories. \n",
    "\n",
    "\ttrue_counts: torch.tensor, shape=(n, ..., L)\n",
    "\t\tA tensor with `n` examples and `L` possible categories.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tloss: float\n",
    "\t\tThe multinomial log likelihood loss of the true counts given the\n",
    "\t\tpredicted probabilities, averaged over all examples and all other\n",
    "\t\tdimensions.\n",
    "\t\"\"\"\n",
    "\n",
    "\tlog_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "\tlog_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "\tlog_prod_exp = torch.sum(true_counts * logps, dim=-1)\n",
    "\treturn -log_fact_sum + log_prod_fact - log_prod_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.py\n",
    "# Author: Jacob Schreiber <jmschreiber91@gmail.com>\n",
    "# Code adapted from Alex Tseng, Avanti Shrikumar, and Ziga Avsec\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "import pyfaidx\n",
    "import pyBigWig\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_meme(filename):\n",
    "    \"\"\"Read a MEME file and return a dictionary of PWMs.\n",
    "\n",
    "    This method takes in the filename of a MEME-formatted file to read in\n",
    "    and returns a dictionary of the PWMs where the keys are the metadata\n",
    "    line and the values are the PWMs.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        The filename of the MEME-formatted file to read in\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    motifs: dict\n",
    "        A dictionary of the motifs in the MEME file.\n",
    "    \"\"\"\n",
    "\n",
    "    motifs = {}\n",
    "\n",
    "    with open(filename, \"r\") as infile:\n",
    "        motif, width, i = None, None, 0\n",
    "\n",
    "        for line in infile:\n",
    "            if motif is None:\n",
    "                if line[:5] == 'MOTIF':\n",
    "                    motif = line.split()[1]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            elif width is None:\n",
    "                if line[:6] == 'letter':\n",
    "                    width = int(line.split()[5])\n",
    "                    pwm = numpy.zeros((width, 4))\n",
    "\n",
    "            elif i < width:\n",
    "                pwm[i] = list(map(float, line.split()))\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                motifs[motif] = pwm\n",
    "                motif, width, i = None, None, 0\n",
    "\n",
    "    return motifs\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence, alphabet=['A', 'C', 'G', 'T'], dtype='int8', \n",
    "    desc=None, verbose=False, **kwargs):\n",
    "    \"\"\"Converts a string or list of characters into a one-hot encoding.\n",
    "\n",
    "    This function will take in either a string or a list and convert it into a\n",
    "    one-hot encoding. If the input is a string, each character is assumed to be\n",
    "    a different symbol, e.g. 'ACGT' is assumed to be a sequence of four \n",
    "    characters. If the input is a list, the elements can be any size.\n",
    "\n",
    "    Although this function will be used here primarily to convert nucleotide\n",
    "    sequences into one-hot encoding with an alphabet of size 4, in principle\n",
    "    this function can be used for any types of sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : str or list\n",
    "        The sequence to convert to a one-hot encoding.\n",
    "\n",
    "    alphabet : set or tuple or list\n",
    "        A pre-defined alphabet where the ordering of the symbols is the same\n",
    "        as the index into the returned tensor, i.e., for the alphabet ['A', 'B']\n",
    "        the returned tensor will have a 1 at index 0 if the character was 'A'.\n",
    "        Characters outside the alphabet are ignored and none of the indexes are\n",
    "        set to 1. Default is ['A', 'C', 'G', 'T'].\n",
    "\n",
    "    dtype : str or numpy.dtype, optional\n",
    "        The data type of the returned encoding. Default is int8.\n",
    "\n",
    "    desc : str or None, optional\n",
    "        The title to display in the progress bar.\n",
    "\n",
    "    verbose : bool or str, optional\n",
    "        Whether to display a progress bar. If a string is passed in, use as the\n",
    "        name of the progressbar. Default is False.\n",
    "\n",
    "    kwargs : arguments\n",
    "        Arguments to be passed into tqdm. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ohe : numpy.ndarray\n",
    "        A binary matrix of shape (alphabet_size, sequence_length) where\n",
    "        alphabet_size is the number of unique elements in the sequence and\n",
    "        sequence_length is the length of the input sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    d = verbose is False\n",
    "    alphabet_lookup = {char: i for i, char in enumerate(alphabet)}\n",
    "\n",
    "    ohe = numpy.zeros((len(sequence), len(alphabet)), dtype=dtype)\n",
    "    for i, char in tqdm(enumerate(sequence), disable=d, desc=desc, **kwargs):\n",
    "        idx = alphabet_lookup.get(char, -1)\n",
    "        if idx != -1:\n",
    "            ohe[i, idx] = 1\n",
    "\n",
    "    return ohe\n",
    "\n",
    "\n",
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    \"\"\"A data generator for BPNet inputs.\n",
    "\n",
    "    This generator takes in an extracted set of sequences, output signals,\n",
    "    and control signals, and will return a single element with random\n",
    "    jitter and reverse-complement augmentation applied. Jitter is implemented\n",
    "    efficiently by taking in data that is wider than the in/out windows by\n",
    "    two times the maximum jitter and windows are extracted from that.\n",
    "    Essentially, if an input window is 1000 and the maximum jitter is 128, one\n",
    "    would pass in data with a length of 1256 and a length 1000 window would be\n",
    "    extracted starting between position 0 and 256. This  generator must be \n",
    "    wrapped by a PyTorch generator object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences: torch.tensor, shape=(n, 4, in_window+2*max_jitter)\n",
    "        A one-hot encoded tensor of `n` example sequences, each of input \n",
    "        length `in_window`. See description above for connection with jitter.\n",
    "\n",
    "    signals: torch.tensor, shape=(n, t, out_window+2*max_jitter)\n",
    "        The signals to predict, usually counts, for `n` examples with\n",
    "        `t` output tasks (usually 2 if stranded, 1 otherwise), each of \n",
    "        output length `out_window`. See description above for connection \n",
    "        with jitter.\n",
    "\n",
    "    controls: torch.tensor, shape=(n, t, out_window+2*max_jitter) or None, optional\n",
    "        The control signal to take as input, usually counts, for `n`\n",
    "        examples with `t` strands and output length `out_window`. If\n",
    "        None, does not return controls.\n",
    "\n",
    "    in_window: int, optional\n",
    "        The input window size. Default is 2114.\n",
    "\n",
    "    out_window: int, optional\n",
    "        The output window size. Default is 1000.\n",
    "\n",
    "    max_jitter: int, optional\n",
    "        The maximum amount of jitter to add, in either direction, to the\n",
    "        midpoints that are passed in. Default is 0.\n",
    "\n",
    "    reverse_complement: bool, optional\n",
    "        Whether to reverse complement-augment half of the data. Default is False.\n",
    "\n",
    "    random_state: int or None, optional\n",
    "        Whether to use a deterministic seed or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequences, signals, controls=None, in_window=2114, \n",
    "        out_window=1000, max_jitter=0, reverse_complement=False, \n",
    "        random_state=None):\n",
    "        self.in_window = in_window\n",
    "        self.out_window = out_window\n",
    "        self.max_jitter = max_jitter\n",
    "\n",
    "        self.reverse_complement = reverse_complement\n",
    "        self.random_state = numpy.random.RandomState(random_state)\n",
    "\n",
    "        self.signals = signals\n",
    "        self.controls = controls\n",
    "        self.sequences = sequences\t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.random_state.choice(len(self.sequences))\n",
    "        j = 0 if self.max_jitter == 0 else self.random_state.randint(self.max_jitter*2) \n",
    "\n",
    "        X = self.sequences[i]\n",
    "        y = self.signals[i]\n",
    "\n",
    "        if self.controls is not None:\n",
    "            X_ctl = self.controls[i]\n",
    "\n",
    "        if self.controls is not None:\n",
    "            return X, X_ctl, y\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def extract_loci(loci, sequences, signals=None, controls=None, chroms=None, \n",
    "    in_window=2114, out_window=1000, max_jitter=0, min_counts=None,\n",
    "    max_counts=None, n_loci=None, verbose=False):\n",
    "    \"\"\"Extract sequences and signals at coordinates from a locus file.\n",
    "\n",
    "    This function will take in genome-wide sequences, signals, and optionally\n",
    "    controls, and extract the values of each at the coordinates specified in\n",
    "    the locus file/s and return them as tensors.\n",
    "\n",
    "    Signals and controls are both lists with the length of the list, n_s\n",
    "    and n_c respectively, being the middle dimension of the returned\n",
    "    tensors. Specifically, the returned tensors of size \n",
    "    (len(loci), n_s/n_c, (out_window/in_wndow)+max_jitter*2).\n",
    "\n",
    "    The values for sequences, signals, and controls, can either be filepaths\n",
    "    or dictionaries of numpy arrays or a mix of the two. When a filepath is \n",
    "    passed in it is loaded using pyfaidx or pyBigWig respectively.   \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loci: str or pandas.DataFrame or list/tuple of such\n",
    "        Either the path to a bed file or a pandas DataFrame object containing\n",
    "        three columns: the chromosome, the start, and the end, of each locus\n",
    "        to train on. Alternatively, a list or tuple of strings/DataFrames where\n",
    "        the intention is to train on the interleaved concatenation, i.e., when\n",
    "        you want to train on peaks and negatives.\n",
    "\n",
    "    sequences: str or dictionary\n",
    "        Either the path to a fasta file to read from or a dictionary where the\n",
    "        keys are the unique set of chromosoms and the values are one-hot\n",
    "        encoded sequences as numpy arrays or memory maps.\n",
    "\n",
    "    signals: list of strs or list of dictionaries or None, optional\n",
    "        A list of filepaths to bigwig files, where each filepath will be read\n",
    "        using pyBigWig, or a list of dictionaries where the keys are the same\n",
    "        set of unique chromosomes and the values are numpy arrays or memory\n",
    "        maps. If None, no signal tensor is returned. Default is None.\n",
    "\n",
    "    controls: list of strs or list of dictionaries or None, optional\n",
    "        A list of filepaths to bigwig files, where each filepath will be read\n",
    "        using pyBigWig, or a list of dictionaries where the keys are the same\n",
    "        set of unique chromosomes and the values are numpy arrays or memory\n",
    "        maps. If None, no control tensor is returned. Default is None. \n",
    "\n",
    "    chroms: list or None, optional\n",
    "        A set of chromosomes to extact loci from. Loci in other chromosomes\n",
    "        in the locus file are ignored. If None, all loci are used. Default is\n",
    "        None.\n",
    "\n",
    "    in_window: int, optional\n",
    "        The input window size. Default is 2114.\n",
    "\n",
    "    out_window: int, optional\n",
    "        The output window size. Default is 1000.\n",
    "\n",
    "    max_jitter: int, optional\n",
    "        The maximum amount of jitter to add, in either direction, to the\n",
    "        midpoints that are passed in. Default is 0.\n",
    "\n",
    "    min_counts: float or None, optional\n",
    "        The minimum number of counts, summed across the length of each example\n",
    "        and across all tasks, needed to be kept. If None, no minimum. Default \n",
    "        is None.\n",
    "\n",
    "    max_counts: float or None, optional\n",
    "        The maximum number of counts, summed across the length of each example\n",
    "        and across all tasks, needed to be kept. If None, no maximum. Default \n",
    "        is None.  \n",
    "\n",
    "    n_loci: int or None, optional\n",
    "        A cap on the number of loci to return. Note that this is not the\n",
    "        number of loci that are considered. The difference is that some\n",
    "        loci may be filtered out for various reasons, and those are not\n",
    "        counted towards the total. If None, no cap. Default is None.\n",
    "\n",
    "    verbose: bool, optional\n",
    "        Whether to display a progress bar while loading. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seqs: torch.tensor, shape=(n, 4, in_window+2*max_jitter)\n",
    "        The extracted sequences in the same order as the loci in the locus\n",
    "        file after optional filtering by chromosome.\n",
    "\n",
    "    signals: torch.tensor, shape=(n, len(signals), out_window+2*max_jitter)\n",
    "        The extracted signals where the first dimension is in the same order\n",
    "        as loci in the locus file after optional filtering by chromosome and\n",
    "        the second dimension is in the same order as the list of signal files.\n",
    "        If no signal files are given, this is not returned.\n",
    "\n",
    "    controls: torch.tensor, shape=(n, len(controls), out_window+2*max_jitter)\n",
    "        The extracted controls where the first dimension is in the same order\n",
    "        as loci in the locus file after optional filtering by chromosome and\n",
    "        the second dimension is in the same order as the list of control files.\n",
    "        If no control files are given, this is not returned.\n",
    "    \"\"\"\n",
    "\n",
    "    seqs, signals_, controls_ = [], [], []\n",
    "    in_width, out_width = in_window // 2, out_window // 2\n",
    "\n",
    "    # Load the sequences\n",
    "    if isinstance(sequences, str):\n",
    "        sequences = pyfaidx.Fasta(sequences)\n",
    "\n",
    "    names = ['chrom', 'start', 'end']\n",
    "    if not isinstance(loci, (tuple, list)):\n",
    "        loci = [loci]\n",
    "\n",
    "    loci_dfs = []\n",
    "    for i, df in enumerate(loci):\n",
    "        if isinstance(df, str):\n",
    "            df = pandas.read_csv(df, sep='\\t', usecols=[0, 1, 2], \n",
    "                header=None, index_col=False, names=names)\n",
    "        elif isinstance(df, pandas.DataFrame):\n",
    "            df = df.iloc[:, [0, 1, 2]].copy()\n",
    "\n",
    "        df['idx'] = numpy.arange(len(df)) * len(loci) + i\n",
    "        loci_dfs.append(df)\n",
    "\n",
    "    loci = pandas.concat(loci_dfs).set_index(\"idx\").sort_index().reset_index(drop=True)\n",
    "\n",
    "    if chroms is not None:\n",
    "        loci = loci[numpy.isin(loci['chrom'], chroms)]\n",
    "    print(\"loci shape\", loci.shape)\n",
    "\n",
    "    # Load the signal and optional control tracks if filenames are given\n",
    "    _signals = []\n",
    "    if signals is not None:\n",
    "        for i, signal in enumerate(signals):\n",
    "            if isinstance(signal, str):\n",
    "                signal = pyBigWig.open(signal)\n",
    "            _signals.append(signal)\n",
    "\n",
    "        signals = _signals\n",
    "\n",
    "\n",
    "    _controls = []\n",
    "    if controls is not None:\n",
    "        for i, control in enumerate(controls):\n",
    "            if control == \"\":\n",
    "                _controls.append(\"\")\n",
    "            else:\n",
    "                if isinstance(control, str):\n",
    "                    control = pyBigWig.open(control, \"r\")\n",
    "                _controls.append(control)\n",
    "\n",
    "        controls = _controls\n",
    "    print(\"done iterating through controls\")\n",
    "\n",
    "    desc = \"Loading Loci\"\n",
    "    d = not verbose\n",
    "\n",
    "    max_width = max(in_width, out_width)\n",
    "    loci_count = 0\n",
    "    for chrom, start, end in tqdm(loci.values, disable=d, desc=desc):\n",
    "        mid = start + (end - start) // 2\n",
    "\n",
    "        if start - max_width - max_jitter < 0:\n",
    "            continue\n",
    "\n",
    "        if end + max_width + max_jitter >= len(sequences[chrom]):\n",
    "            continue\n",
    "\n",
    "        if n_loci is not None and loci_count == n_loci:\n",
    "            break \n",
    "\n",
    "        start = mid - out_width - max_jitter\n",
    "        end = mid + out_width + max_jitter\n",
    "\n",
    "        # Extract the signal from each of the signal files\n",
    "        if signals is not None:\n",
    "            signals_.append([])\n",
    "            for signal in signals:\n",
    "                if isinstance(signal, dict):\n",
    "                    signal_ = signal[chrom][start:end]\n",
    "                else:\n",
    "                    signal_ = signal.values(chrom, start, end, numpy=True)\n",
    "                    signal_ = numpy.nan_to_num(signal_)\n",
    "\n",
    "                signals_[-1].append(signal_)\n",
    "\n",
    "        # For the sequences and controls extract a window the size of the input\n",
    "        '''start = mid - in_width - max_jitter\n",
    "        end = mid + in_width + max_jitter'''\n",
    "\n",
    "        # Extract the controls from each of the control files\n",
    "        if controls is not None:\n",
    "            controls_.append([])\n",
    "            for control in controls:\n",
    "                if isinstance(control, dict):\n",
    "                    control_ = control[chrom][start:end]\n",
    "                elif control == \"\":\n",
    "                    control_ = numpy.zeros(end - start)\n",
    "                else:\n",
    "                    control_ = control.values(chrom, start, end, numpy=True)\n",
    "                    control_ = numpy.nan_to_num(control_)\n",
    "\n",
    "                controls_[-1].append(control_)\n",
    "\n",
    "        # Extract the sequence\n",
    "        if isinstance(sequences, dict):\n",
    "            seq = sequences[chrom][start:end].T\n",
    "        else:\n",
    "            seq = one_hot_encode(sequences[chrom][start:end].seq.upper(),\n",
    "                alphabet=['A', 'C', 'G', 'T']).T\n",
    "\n",
    "        seqs.append(seq)\n",
    "        loci_count += 1\n",
    "    print('done with chrom, start, end')\n",
    "    seqs = torch.tensor(numpy.array(seqs), dtype=torch.float32)\n",
    "\n",
    "    if signals is not None:\n",
    "        signals_ = torch.tensor(numpy.array(signals_), dtype=torch.float32)\n",
    "\n",
    "        idxs = torch.ones(signals_.shape[0], dtype=torch.bool)\n",
    "        if max_counts is not None:\n",
    "            idxs = (idxs) & (signals_.sum(dim=(1, 2)) < max_counts)\n",
    "        if min_counts is not None:\n",
    "            idxs = (idxs) & (signals_.sum(dim=(1, 2)) > min_counts)\n",
    "\n",
    "        if controls is not None:\n",
    "            controls_ = torch.tensor(numpy.array(controls_), dtype=torch.float32)\n",
    "            return seqs[idxs], signals_[idxs], controls_[idxs]\n",
    "\n",
    "        return seqs[idxs], signals_[idxs]\n",
    "    else:\n",
    "        if controls is not None:\n",
    "            controls_ = torch.tensor(numpy.array(controls_), dtype=torch.float32)\n",
    "            return seqs, controls_\n",
    "\n",
    "        return seqs\n",
    "\n",
    "def extract_signals(loci, sequences, signals=None, controls=None, chroms=None, \n",
    "    in_window=2114, out_window=1000, max_jitter=0, min_counts=None,\n",
    "    max_counts=None, n_loci=None, verbose=False):\n",
    "\n",
    "    # print(loci)\n",
    "    # print(signals)\n",
    "\n",
    "    seqs, signals_, controls_ = [], [], []\n",
    "    in_width, out_width = in_window // 2, out_window // 2\n",
    "\n",
    "\n",
    "    names = ['chrom', 'start', 'end']\n",
    "    if not isinstance(loci, (tuple, list)):\n",
    "        loci = [loci]\n",
    "\n",
    "    loci_dfs = []\n",
    "    for i, df in enumerate(loci):\n",
    "        if isinstance(df, str):\n",
    "            try:\n",
    "                df = pandas.read_csv(df, sep='\\t', usecols=[0, 1, 2], \n",
    "                    header=None, index_col=False, names=names)\n",
    "            except:\n",
    "                print(\"File Doesn't Exist!\")\n",
    "                return\n",
    "            df['idx'] = numpy.arange(len(df)) * len(loci) + i\n",
    "        loci_dfs.append(df)\n",
    "\n",
    "    loci = pandas.concat(loci_dfs).set_index(\"idx\").sort_index().reset_index(drop=True)\n",
    "\n",
    "    if chroms is not None:\n",
    "        loci = loci[numpy.isin(loci['chrom'], chroms)]\n",
    "\n",
    "    # Load the signal and optional control tracks if filenames are given\n",
    "    _signals = []\n",
    "    if signals is not None:\n",
    "        for i, signal in enumerate(signals):\n",
    "            if isinstance(signal, str):\n",
    "                try:\n",
    "                    signal = pyBigWig.open(signal)\n",
    "                except:\n",
    "                    print(\"Null File\")\n",
    "                    return\n",
    "            _signals.append(signal)\n",
    "\n",
    "        signals = _signals\n",
    "\n",
    "    _controls = []\n",
    "    if controls is not None:\n",
    "        for i, control in enumerate(controls):\n",
    "            if isinstance(control, str):\n",
    "                control = pyBigWig.open(control, \"r\")\n",
    "            _controls.append(control)\n",
    "\n",
    "        controls = _controls\n",
    "\n",
    "    desc = \"Loading Loci\"\n",
    "    d = not verbose\n",
    "\n",
    "    max_width = max(in_width, out_width)\n",
    "    loci_count = 0\n",
    "\n",
    "    # print(loci)\n",
    "    # print(loci.values)\n",
    "    for chrom, start, end in tqdm(loci.values, disable=d, desc=desc):\n",
    "        mid = start + (end - start) // 2\n",
    "\n",
    "        if start - max_width - max_jitter < 0:\n",
    "            continue\n",
    "\n",
    "        if n_loci is not None and loci_count == n_loci:\n",
    "            break \n",
    "\n",
    "        start = mid - out_width - max_jitter\n",
    "        end = mid + out_width + max_jitter\n",
    "\n",
    "        # Extract the signal from each of the signal files\n",
    "        if signals is not None:\n",
    "            signals_.append([])\n",
    "            for signal in signals:\n",
    "                if isinstance(signal, dict):\n",
    "                    signal_ = signal[chrom][start:end]\n",
    "                else:\n",
    "                    try:\n",
    "                        signal_ = signal.values(chrom, start, end, numpy=True)\n",
    "                        signal_ = numpy.nan_to_num(signal_)\n",
    "                    except:\n",
    "                        print(\"error with interval bounds\")\n",
    "                        print(signal)\n",
    "                        print(chrom)\n",
    "                        print(start)\n",
    "                        print(end)\n",
    "                        print(signals_)\n",
    "\n",
    "                signals_[-1].append(signal_)\n",
    "\n",
    "        # For the sequences and controls extract a window the size of the input\n",
    "        start = mid - in_width - max_jitter\n",
    "        end = mid + in_width + max_jitter\n",
    "\n",
    "        # Extract the controls from each of the control files\n",
    "        if controls is not None:\n",
    "            controls_.append([])\n",
    "            for control in controls:\n",
    "                if isinstance(control, dict):\n",
    "                    control_ = control[chrom][start:end]\n",
    "                else:\n",
    "                    control_ = control.values(chrom, start, end, numpy=True)\n",
    "                    control_ = numpy.nan_to_num(control_)\n",
    "\n",
    "                controls_[-1].append(control_)\n",
    "\n",
    "        loci_count += 1\n",
    "\n",
    "    if signals is not None:\n",
    "        signals_ = torch.tensor(numpy.array(signals_), dtype=torch.float32)\n",
    "\n",
    "        idxs = torch.ones(signals_.shape[0], dtype=torch.bool)\n",
    "        if max_counts is not None:\n",
    "            idxs = (idxs) & (signals_.sum(dim=(1, 2)) < max_counts)\n",
    "        if min_counts is not None:\n",
    "            idxs = (idxs) & (signals_.sum(dim=(1, 2)) > min_counts)\n",
    "\n",
    "        return signals_[idxs]\n",
    "\n",
    "\n",
    "def PeakGenerator(loci, sequences, signals, controls=None, chroms=None, \n",
    "    in_window=2114, out_window=1000, max_jitter=0, reverse_complement=True, \n",
    "    min_counts=None, max_counts=None, random_state=None, pin_memory=True, \n",
    "    num_workers=0, batch_size=32, verbose=False):\n",
    "    \"\"\"This is a constructor function that handles all IO.\n",
    "\n",
    "    This function will extract signal from all signal and control files,\n",
    "    pass that into a DataGenerator, and wrap that using a PyTorch data\n",
    "    loader. This is the only function that needs to be used.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loci: str or pandas.DataFrame or list/tuple of such\n",
    "        Either the path to a bed file or a pandas DataFrame object containing\n",
    "        three columns: the chromosome, the start, and the end, of each locus\n",
    "        to train on. Alternatively, a list or tuple of strings/DataFrames where\n",
    "        the intention is to train on the interleaved concatenation, i.e., when\n",
    "        you want ot train on peaks and negatives.\n",
    "\n",
    "    sequences: str or dictionary\n",
    "        Either the path to a fasta file to read from or a dictionary where the\n",
    "        keys are the unique set of chromosoms and the values are one-hot\n",
    "        encoded sequences as numpy arrays or memory maps.\n",
    "\n",
    "    signals: list of strs or list of dictionaries\n",
    "        A list of filepaths to bigwig files, where each filepath will be read\n",
    "        using pyBigWig, or a list of dictionaries where the keys are the same\n",
    "        set of unique chromosomes and the values are numpy arrays or memory\n",
    "        maps.\n",
    "\n",
    "    controls: list of strs or list of dictionaries or None, optional\n",
    "        A list of filepaths to bigwig files, where each filepath will be read\n",
    "        using pyBigWig, or a list of dictionaries where the keys are the same\n",
    "        set of unique chromosomes and the values are numpy arrays or memory\n",
    "        maps. If None, no control tensor is returned. Default is None. \n",
    "\n",
    "    chroms: list or None, optional\n",
    "        A set of chromosomes to extact loci from. Loci in other chromosomes\n",
    "        in the locus file are ignored. If None, all loci are used. Default is\n",
    "        None.\n",
    "\n",
    "    in_window: int, optional\n",
    "        The input window size. Default is 2114.\n",
    "\n",
    "    out_window: int, optional\n",
    "        The output window size. Default is 1000.\n",
    "\n",
    "    max_jitter: int, optional\n",
    "        The maximum amount of jitter to add, in either direction, to the\n",
    "        midpoints that are passed in. Default is 128.\n",
    "\n",
    "    reverse_complement: bool, optional\n",
    "        Whether to reverse complement-augment half of the data. Default is True.\n",
    "\n",
    "    min_counts: float or None, optional\n",
    "        The minimum number of counts, summed across the length of each example\n",
    "        and across all tasks, needed to be kept. If None, no minimum. Default \n",
    "        is None.\n",
    "\n",
    "    max_counts: float or None, optional\n",
    "        The maximum number of counts, summed across the length of each example\n",
    "        and across all tasks, needed to be kept. If None, no maximum. Default \n",
    "        is None.  \n",
    "\n",
    "    random_state: int or None, optional\n",
    "        Whether to use a deterministic seed or not.\n",
    "\n",
    "    pin_memory: bool, optional\n",
    "        Whether to pin page memory to make data loading onto a GPU easier.\n",
    "        Default is True.\n",
    "\n",
    "    num_workers: int, optional\n",
    "        The number of processes fetching data at a time to feed into a model.\n",
    "        If 0, data is fetched from the main process. Default is 0.\n",
    "\n",
    "    batch_size: int, optional\n",
    "        The number of data elements per batch. Default is 32.\n",
    "\n",
    "    verbose: bool, optional\n",
    "        Whether to display a progress bar while loading. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: torch.utils.data.DataLoader\n",
    "        A PyTorch DataLoader wrapped DataGenerator object.\n",
    "    \"\"\"\n",
    "\n",
    "    X = extract_loci(loci=loci, sequences=sequences, signals=signals, \n",
    "        controls=controls, chroms=chroms, in_window=in_window, \n",
    "        out_window=out_window, max_jitter=max_jitter, min_counts=min_counts,\n",
    "        max_counts=max_counts, verbose=verbose)\n",
    "\n",
    "    if controls is not None:\n",
    "        sequences, signals_, controls_ = X\n",
    "    else:\n",
    "        sequences, signals_ = X\n",
    "        controls_ = None\n",
    "\n",
    "    X_gen = DataGenerator(sequences, signals_, controls=controls_, \n",
    "        in_window=in_window, out_window=out_window, max_jitter=max_jitter,\n",
    "        reverse_complement=reverse_complement, random_state=random_state)\n",
    "\n",
    "    X_gen = torch.utils.data.DataLoader(X_gen, pin_memory=pin_memory,\n",
    "        num_workers=num_workers, batch_size=batch_size) \n",
    "\n",
    "    return X_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# from datafunctions_withprofile import extract_loci, PeakGenerator\n",
    "#from DefinitelyNotBPNet import DefinitelyNotBPNet\n",
    "\n",
    "root = \"/users/myin25/projects/celltype_specificity/\"\n",
    "cell_types = ['K562', 'CACO2', 'A673', 'HUVEC']\n",
    "histone_types = ['H3K9me3', 'H3K4me1', 'H3K27me3', 'H3K27ac', 'H3K36me3', 'H3K4me3', 'H3K9ac', 'H3K79me2']\n",
    "sequences = root + \"refs/hg38.fasta\"\n",
    "peaks = root + \"data/procap/union_peaks_fold1_train.bed.gz\"\n",
    "peaks_val = root + \"data/procap/union_peaks_fold1_val.bed.gz\"\n",
    "\n",
    "histonefolders = {celltype: root + \"data/{}\".format(celltype) for celltype in cell_types}\n",
    "\n",
    "def getfoldchangebigwig(cell, histone):\n",
    "    if ((cell == 'CACO2' and (histone == 'H3K27ac' or histone == 'H3K9ac' or histone == 'H3K79me2')) or\n",
    "    (cell == 'A673' and (histone == 'H3K9ac' or histone == 'H3K79me2'))):\n",
    "        return \"\"\n",
    "    return histonefolders[cell] + '/{}/foldchange.bigWig'.format(histone)\n",
    "\n",
    "# Paths to different histones\n",
    "\n",
    "\n",
    "levels_actual_path = {'K562' : [root + \"data/procap/observed/K562/5prime.neg.bigWig\", root + \"data/procap/observed/K562/5prime.pos.bigWig\"],\n",
    "                       'CACO2' : [root + \"data/procap/observed/CACO2/5prime.neg.bigWig\", root + \"data/procap/observed/CACO2/5prime.pos.bigWig\"],\n",
    "                       'A673' : [root + \"data/procap/observed/A673/5prime.neg.bigWig\", root + \"data/procap/observed/A673/5prime.pos.bigWig\"],\n",
    "                       'HUVEC' : [root + \"data/procap/observed/HUVEC/5prime.neg.bigWig\", root + \"data/procap/observed/HUVEC/5prime.pos.bigWig\"]}\n",
    "\n",
    "# peaks = root + \"data/procap/observed/K562/peaks_fold1_train.bed.gz\"\n",
    "# peaks_val = root + \"data/procap/observed/K562/peaks_fold1_val.bed.gz\"\n",
    "\n",
    "training_chroms = ['chr{}'.format(i) for i in range(1, 23)]\n",
    "training_chroms.append('chrX')\n",
    "training_chroms.append('chrY')\n",
    "valid_chroms = ['chr{}'.format(i) for i in range(1, 23)]\n",
    "valid_chroms.append('chrX')\n",
    "valid_chroms.append('chrY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = 'K562'\n",
    "y_train = levels_actual_path[cell]\n",
    "\n",
    "signalpaths = []\n",
    "for histone in histone_types:\n",
    "    if not ((cell == 'CACO2' and (histone == 'H3K27ac' or histone == 'H3K9ac' or histone == 'H3K79me2')) or\n",
    "    (cell == 'A673' and (histone == 'H3K9ac' or histone == 'H3K79me2'))):\n",
    "        signalpaths.append(getfoldchangebigwig(cell, histone))\n",
    "    else:\n",
    "        signalpaths.append(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loci shape (77109, 3)\n",
      "done iterating through controls\n",
      "done with chrom, start, end\n"
     ]
    }
   ],
   "source": [
    "training_data = PeakGenerator(peaks, sequences, y_train, signalpaths, chroms=training_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getallhistonesignals(signalpaths):\n",
    "    _, y_valid, controls = extract_loci(loci=peaks_val, sequences=sequences, signals=y_train, controls=signalpaths, chroms=valid_chroms)\n",
    "\n",
    "    return y_valid, controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loci shape (14269, 3)\n",
      "done iterating through controls\n",
      "done with chrom, start, end\n"
     ]
    }
   ],
   "source": [
    "y_valid, X_valid = getallhistonesignals(signalpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DefinitelyNotBPNet(8).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(training_data, optimizer, X_valid=X_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the expected MSE when predicting just using the average\n",
    "_, train_signalss__ = extract_loci(loci=peaks, sequences=sequences, signals=y_train, controls=None, chroms=training_chroms)\n",
    "\n",
    "\n",
    "# Read in signals, take sum across axis to get average log observed value\n",
    "train_signalss__ = extract_signals(loci=peaks, sequences=sequences, signals=y_train, controls=None, chroms=training_chroms)\n",
    "\n",
    "train_signalss__ = torch.sum(train_signalss__, (1, 2))\n",
    "print(train_signalss__)\n",
    "\n",
    "train_signalss__ = torch.log(train_signalss__ + 1)\n",
    "print(train_signalss__)\n",
    "\n",
    "plt.hist(train_signalss__, bins=20)\n",
    "print(torch.mean(train_signalss__))\n",
    "\n",
    "# Calculate the mse\n",
    "avgmse = torch.fill_(torch.empty((14269)), 2.6262)\n",
    "\n",
    "# Compare the logs of the two datasets\n",
    "count_mse = torch.square(torch.log(y_valid + 1) - avgmse)\n",
    "count_mse = torch.mean(count_mse.squeeze(), dim=-1)\n",
    "print(count_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:copro] *",
   "language": "python",
   "name": "conda-env-copro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
